{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Model_evaluation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f175GuBHOMn",
        "outputId": "a51dc043-09f1-438e-847d-d0dedbe5bc89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 895 kB 6.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 49.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 41.6 MB/s \n",
            "\u001b[?25h  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.19.0.dev0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 12.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.5.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.11.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.49)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120747 sha256=1b1f09e8216174a3ea100aca4aa3ef78570f0e95cf10fb8723ec5a6c1f3bf683\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.0 sentencepiece-0.1.96\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "% cd /content/drive/MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XwXs2x5HUhd",
        "outputId": "ba38319b-b8fc-41ca-e515-0edda2e0b89f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sentence_transformers.cross_encoder import CrossEncoder\n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
        "from torch.utils.data import DataLoader\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatiser = WordNetLemmatizer()\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lzl3sm6XHWsR",
        "outputId": "32fd631a-1f5d-474a-f934-18e7bdaf6704"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# don't convert to lowercase as using bert base cased (case is important)\n",
        "def preprocess(text):\n",
        "    res = re.sub(r'[^\\w\\s]', '',text) # Remove punctuations\n",
        "    res = \" \".join(res.split())       # Remove whitespaces\n",
        "    tokens = word_tokenize(res) \n",
        "    new_tokens = [lemmatiser.lemmatize(i) for i in tokens if not i in stop_words] # Remove stopwords and lemmatise\n",
        "    res = ' '.join(new_tokens)\n",
        "    return res"
      ],
      "metadata": {
        "id": "rgbE_KKuAWgx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"eval_texts.json\", \"r\") as fp:\n",
        "  eval_texts = json.load(fp)\n",
        "for i in eval_texts:\n",
        "    eval_texts[i] = preprocess(eval_texts[i])\n",
        "def evaluate(name, label):\n",
        "  df = pd.read_csv(name)\n",
        "  l1 = []\n",
        "  l2 = []\n",
        "  l3 = []\n",
        "  for index,row in df.iterrows():\n",
        "      id1,id2 = row['pair_id'].split('_')\n",
        "      if id1 in eval_texts and id2 in eval_texts:\n",
        "         l1.append(eval_texts[id1])\n",
        "         l2.append(eval_texts[id2])\n",
        "         l3.append((row[label]-1)/3)\n",
        "  di = {'Text 1':l1, 'Text 2':l2, 'GT': l3}\n",
        "  df = pd.DataFrame(di)\n",
        "  test_examples=[]\n",
        "  score=[]\n",
        "  for i in range(len(df)):\n",
        "    test_examples.append([df[\"Text 1\"][i], df[\"Text 2\"][i]])\n",
        "    score.append(df[\"GT\"][i])\n",
        "    \n",
        "  return(test_examples,score)"
      ],
      "metadata": {
        "id": "5EvSF9maHdqv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict = {}\n",
        "all_labels = [\"GEO\",\"ENT\",\"TIME\",\"NAR\",\"Overall\",\"STYLE\",\"TONE\"]"
      ],
      "metadata": {
        "id": "rk0BFwObHeNQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data,score=evaluate(\"final_evaluation_data.csv\", all_labels[0])\n",
        "model1=pickle.load(open(\"/content/drive/MyDrive/Weights_final/Geography/bert-base-multilingual-cased_8_2e-05_MSE_6\", 'rb'))\n",
        "scores_predicted=model1.predict(data)\n",
        "\n",
        "dict[all_labels[0]] = scores_predicted\n",
        "\n",
        "data,score=evaluate(\"final_evaluation_data.csv\", all_labels[1])\n",
        "model1=pickle.load(open(\"/content/drive/MyDrive/Weights_final/Entities/bert-base-multilingual-cased_8_2e-05_MSE_2\", 'rb'))\n",
        "scores_predicted=model1.predict(data)\n",
        "\n",
        "dict[all_labels[1]] = scores_predicted\n",
        "\n",
        "data,score=evaluate(\"final_evaluation_data.csv\", all_labels[2])\n",
        "model1=pickle.load(open(\"/content/drive/MyDrive/Weights_final/Time/bert-base-multilingual-cased_8_2e-05_MSE_6\", 'rb'))\n",
        "scores_predicted=model1.predict(data)\n",
        "\n",
        "dict[all_labels[2]] = scores_predicted\n",
        "\n",
        "data,score=evaluate(\"final_evaluation_data.csv\", all_labels[3])\n",
        "model1=pickle.load(open(\"/content/drive/MyDrive/Weights_final/Narrative/bert-base-multilingual-cased_8_2e-05_MSE_3\", 'rb'))\n",
        "scores_predicted=model1.predict(data)\n",
        "\n",
        "dict[all_labels[3]] = scores_predicted\n",
        "\n",
        "data,score=evaluate(\"final_evaluation_data.csv\", all_labels[5])\n",
        "model1=pickle.load(open(\"/content/drive/MyDrive/Weights_final/Style/bert-base-multilingual-cased_8_2e-05_MSE_2\", 'rb'))\n",
        "scores_predicted=model1.predict(data)\n",
        "\n",
        "dict[all_labels[5]] = scores_predicted\n",
        "\n",
        "data,score=evaluate(\"final_evaluation_data.csv\", all_labels[6])\n",
        "model1=pickle.load(open(\"/content/drive/MyDrive/Weights_final/Tone/bert-base-multilingual-cased_8_2e-05_MSE_1\", 'rb'))\n",
        "scores_predicted=model1.predict(data)\n",
        "\n",
        "dict[all_labels[6]] = scores_predicted\n"
      ],
      "metadata": {
        "id": "OrMtiOJXHfw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data,score=evaluate(\"final_evaluation_data.csv\", all_labels[4])\n",
        "dict[all_labels[4]] = score"
      ],
      "metadata": {
        "id": "U2hvQEFpHh7o"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd                                                             #Saving the predicted_scores for all 6 models\n",
        "df = pd.DataFrame(dict)\n",
        "df.to_csv('temp.csv')"
      ],
      "metadata": {
        "id": "08PA9gJEHkvl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np                                                              #Preparing features for evaluation\n",
        "\n",
        "df = pd.read_csv(\"temp.csv\")\n",
        "eval = np.array(df['GEO']).reshape(-1,1)\n",
        "eval = np.append(eval,np.array(df['ENT']).reshape(-1,1),axis = 1)\n",
        "eval = np.append(eval,np.array(df['TIME']).reshape(-1,1),axis = 1)\n",
        "eval = np.append(eval,np.array(df['NAR']).reshape(-1,1),axis = 1)\n",
        "eval = np.append(eval,np.array(df['STYLE']).reshape(-1,1),axis = 1)\n",
        "eval = np.append(eval,np.array(df['TONE']).reshape(-1,1),axis = 1)\n",
        "label = np.array(df['Overall']).reshape(-1,1)\n",
        "label = label.reshape(-1,)"
      ],
      "metadata": {
        "id": "fnO5EHHkHpJc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as T\n",
        "class Net(T.nn.Module):                                                         #mlp definition\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.hid1 = T.nn.Linear(6, 10)  # 8-(10-10)-1\n",
        "    self.hid2 = T.nn.Linear(10, 10)\n",
        "    self.oupt = T.nn.Linear(10, 1)\n",
        "\n",
        "    T.nn.init.xavier_uniform_(self.hid1.weight)\n",
        "    T.nn.init.zeros_(self.hid1.bias)\n",
        "    T.nn.init.xavier_uniform_(self.hid2.weight)\n",
        "    T.nn.init.zeros_(self.hid2.bias)\n",
        "    T.nn.init.xavier_uniform_(self.oupt.weight)\n",
        "    T.nn.init.zeros_(self.oupt.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    z = T.relu(self.hid1(x))\n",
        "    z = T.relu(self.hid2(z))\n",
        "    z = self.oupt(z)  # no activation\n",
        "    return z"
      ],
      "metadata": {
        "id": "fGOD_-D2Hxbm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "model = pickle.load(open(\"NN_model\",'rb'))\n",
        "pred = model(T.tensor(eval.astype(np.float32)))\n",
        "loss_f = T.nn.MSELoss()\n",
        "loss = 0\n",
        "for i in range(len(pred)):\n",
        "    loss = loss + loss_f(pred[i],T.tensor(label[i])).item()\n",
        "\n",
        "print(loss/len(pred))\n",
        "data,score=evaluate(\"final_evaluation_data.csv\", all_labels[4])\n",
        "model1=pickle.load(open(\"mbert_model.sav\", 'rb'))                #specify model name\n",
        "pred1=model1.predict(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PU5snRNoHyJJ",
        "outputId": "bf4cecee-d528-4f08-e77d-89fc86af4fbe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1305687193451751\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dVApB_f6vZs",
        "outputId": "94c697ae-8773-4b61-fcae-1409fd948b0c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1680\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred=pred.reshape(-1)\n",
        "pred1=pred1.reshape(-1)\n",
        "\n",
        "x1=[]\n",
        "for i in range(len(pred)):\n",
        "  x1.append((pred[i]+pred1[i])/2)                                               #Averaging results from current model and mbert output\n",
        "x=torch.Tensor([x1,df[\"Overall\"]])\n",
        "coeff=torch.corrcoef(x)\n",
        "print(coeff)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJB5iuRIH4HQ",
        "outputId": "175e79ac-b024-4f4c-efc6-ceed32b1f5ff"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.7020],\n",
            "        [0.7020, 1.0000]])\n"
          ]
        }
      ]
    }
  ]
}